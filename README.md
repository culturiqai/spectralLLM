# SpectralLLM: Experimental sub-20ms structural NLP model achieving 0.8 entity coherence on consumer hardware

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Apple Silicon](https://img.shields.io/badge/Apple%20Silicon-Optimized-black.svg)](https://developer.apple.com/metal/)

> **Revolutionary approach to language modeling using spectral decomposition and wavelet transforms**

## üåä Overview

SpectralLLM introduces an experimental paradigm for language modeling by representing tokens as **superpositions of frequency components** rather than traditional embedding vectors. Our research demonstrates that frequency-domain processing can capture sophisticated linguistic structures, achieving **0.8 entity coherence** while revealing fundamental insights about language representation.

### Key Innovation: Spectral Token Embeddings

Instead of learning direct embedding vectors, SpectralLLM learns **frequency amplitudes and phases** for harmonic bases:

```python
# Traditional: token ‚Üí embedding vector
embedding = lookup_table[token_id]

# SpectralLLM: token ‚Üí frequency decomposition ‚Üí embedding
amplitudes = frequency_amplitudes[token_id]
phases = frequency_phases[token_id] 
embedding = Œ£ amplitudes[i] * sin(2œÄ * frequencies[i] * t + phases[i])
```

## üéØ Key Results

| Metric | Performance | Significance |
|--------|-------------|--------------|
| **Entity Coherence** | **0.8** | Excellent structural understanding |
| **Training Perplexity** | 62K ‚Üí 10-50 | Strong learning dynamics |
| **Model Scales** | 8M, 18M, 40M params | Scalable architecture |
| **Hardware** | M4 Pro optimized | Consumer hardware viable |

### Performance Highlights

- ‚úÖ **0.8 Entity Coherence**: Demonstrates sophisticated grammatical structure capture
- ‚úÖ **Dramatic PPL Improvement**: 62K ‚Üí 10-50 during training shows effective learning
- ‚úÖ **Multi-Scale Success**: Tested on 8M, 18M, and 40M parameter models
- ‚úÖ **Apple Silicon Optimized**: Full MPS acceleration on M4 Pro

## üèóÔ∏è Architecture

### 1. Spectral Embedding Layer
```python
class SpectralEmbedding(nn.Module):
    """Represents tokens as superpositions of frequency components"""
    def __init__(self, vocab_size, embed_dim, harmonic_bases=16):
        self.frequency_amplitudes = nn.Parameter(torch.randn(vocab_size, harmonic_bases))
        self.frequency_phases = nn.Parameter(torch.randn(vocab_size, harmonic_bases))
        self.frequencies = torch.linspace(0.1, math.pi, harmonic_bases)
```

### 2. Hybrid Architecture Strategy
- **Learnable mixing** between traditional and spectral embeddings
- **Smooth transition** from stable training to spectral innovation
- **Adaptive basis selection** for optimal frequency representation

### 3. Wavelet Transformer Blocks
- **Multi-resolution analysis** using PyWavelets integration
- **Learnable wavelet filters** (db4, sym4, dmey families)
- **Efficient decomposition** with proper boundary handling

## üöÄ Quick Start

### Installation

```bash
git clone https://github.com/yourusername/spectralllm.git
cd spectralllm
pip install -r requirements.txt
```

### Training on WikiText-103

```bash
# Train 8M parameter model
python docs/test_subject/spectral-transformer/trainer.md \
    --embed_dim 256 \
    --num_layers 4 \
    --batch_size 16 \
    --use_signal_embed \
    --use_wavelet_attention \
    --use_mps

# Train larger 40M parameter model
python docs/test_subject/spectral-transformer/trainer.md \
    --embed_dim 512 \
    --num_layers 8 \
    --hidden_dim 2048 \
    --batch_size 8 \
    --use_signal_embed \
    --use_wavelet_attention \
    --use_mps
```

### Evaluation

```bash
# Rigorous evaluation with coherence metrics
python docs/test_subject/spectral-transformer/evaluate.md \
    --checkpoint_path ./outputs/best_model.pt \
    --baseline_model gpt2
```

## üìä Detailed Results

### Training Dynamics
- **Initial Perplexity**: ~62,000 (random initialization)
- **Final Training PPL**: 10-50 (dramatic improvement)
- **Validation PPL**: Stable (indicates learning without severe overfitting)

### Coherence Analysis
| Coherence Type | Score | Interpretation |
|----------------|-------|----------------|
| **Entity** | 0.8 | Excellent pronoun resolution and entity tracking |
| **Semantic** | 0.1-0.15 | Lower sentence-to-sentence meaning flow |
| **Discourse** | Variable | Depends on text complexity |
| **Lexical** | Variable | Vocabulary consistency metrics |

### Hardware Performance (M4 Pro)
- **Memory Usage**: Efficient with 24GB RAM
- **Training Speed**: Competitive with MPS acceleration
- **Scalability**: Successfully trained up to 40M parameters

## üî¨ Research Insights

### Frequency-Domain Language Structure

Our results reveal a fundamental insight: **language structure naturally separates in frequency domain**:

- **High Entity Coherence (0.8)**: Suggests spectral representations excel at capturing **syntactic/structural** patterns
- **Lower Semantic Coherence (0.1-0.15)**: Indicates **semantic relationships** may require different frequency bands or hybrid approaches

### Implications for NLP

1. **Structural Tasks**: Spectral methods may be superior for parsing, entity recognition, grammatical analysis
2. **Efficient Embeddings**: Frequency-based representations could be more parameter-efficient
3. **Hybrid Architectures**: Combining spectral structure with traditional semantics

## üõ†Ô∏è Technical Implementation

### Core Components

```python
# Spectral embedding with learnable frequencies
spectral_embed = SpectralEmbedding(vocab_size, embed_dim, harmonic_bases=32)

# Hybrid mixing for training stability
hybrid_embed = HybridEmbedding(vocab_size, embed_dim, spectral_ratio=0.5)

# Wavelet attention mechanism
wavelet_attn = WaveletAttention(embed_dim, num_heads, wavelet_type='db4')
```

### Hardware Optimizations

- **Apple Silicon MPS**: Full Metal Performance Shaders acceleration
- **Memory Efficient**: Optimized for consumer hardware (24GB RAM)
- **Scalable Training**: Supports 8M to 40M+ parameter models

## üìà Benchmarks

### WikiText-103 Results

| Model Size | Parameters | Training PPL | Entity Coherence | Training Time |
|------------|------------|--------------|------------------|---------------|
| Small | 8M | 62K ‚Üí 50 | 0.8 | ~6 hours |
| Medium | 18M | 62K ‚Üí 25 | 0.8 | ~12 hours |
| Large | 40M | 62K ‚Üí 10 | 0.8 | ~24 hours |

*Trained on MacBook Pro M4 Pro (24GB RAM, 16-core GPU)*

## üîÆ Future Directions

### Immediate Research
1. **Frequency Band Analysis**: Investigate which bands capture syntax vs semantics
2. **Semantic Enhancement**: Hybrid approaches for improved semantic coherence
3. **Scale Testing**: Larger models (100M+ parameters)

### Applications
- **Structural NLP**: Enhanced parsing and grammatical analysis
- **Efficient LLMs**: Parameter-efficient language models
- **Domain-Specific**: Scientific text processing, code analysis

## üìö Citation

```bibtex
@article{spectralllm2024,
  title={SpectralLLM: Frequency-Domain Language Modeling with Wavelet Transforms},
  author={Your Name},
  journal={arXiv preprint},
  year={2024},
  note={Achieving 0.8 entity coherence through spectral token embeddings}
}
```

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Areas for Contribution
- **Frequency Analysis**: Tools for analyzing learned frequency patterns
- **New Wavelets**: Additional wavelet families and adaptive selection
- **Evaluation Metrics**: Enhanced coherence and linguistic analysis
- **Hardware Support**: CUDA optimizations, distributed training

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **PyWavelets Team**: For excellent wavelet transform library
- **Hugging Face**: For transformers and datasets infrastructure
- **Apple**: For Metal Performance Shaders enabling M4 Pro optimization
- **Research Community**: For foundational work in spectral analysis and NLP

## üìû Contact

- Email : aditya.tiwari.jsr@gmail.com

---

<div align="center">

**üåä Transforming Language Understanding Through Spectral Analysis üåä**

[Documentation](docs/) ‚Ä¢ [Examples](examples/) ‚Ä¢ [Paper](paper.pdf) ‚Ä¢ [Demo](demo/)

</div> 
